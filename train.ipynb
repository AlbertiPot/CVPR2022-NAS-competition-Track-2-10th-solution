{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Codes\n",
    "## Implementation\n",
    "MLP + ranking based loss\n",
    "## Two ways to reproduce the training procedure\n",
    "1. directly running the bash scripts\n",
    "2. running the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=8, cos=False, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='train_final', seed=4, train_ratio=0.7, val_interval=1, weight_decay=0.0006)\n",
      "start to process task cplfw_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.7015 ktau: -0.0664\n",
      "[Validate] Epoch 1: ktau: 0.1667\n",
      "[Train] Epoch 100: Loss: 0.5932 ktau: 0.3854\n",
      "[Validate] Epoch 100: ktau: 0.2698\n",
      "[Train] Epoch 200: Loss: 0.5742 ktau: 0.4086\n",
      "[Validate] Epoch 200: ktau: 0.3254\n",
      "[Train] Epoch 300: Loss: 0.5554 ktau: 0.4286\n",
      "[Validate] Epoch 300: ktau: 0.3351\n",
      "[Train] Epoch 400: Loss: 0.5661 ktau: 0.4070\n",
      "[Validate] Epoch 400: ktau: 0.2778\n",
      "Best train KTau: 0.395096@epoch 348 on task cplfw_rank\n",
      "Namespace(batch_size=25, cos=True, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='train_final', seed=4, train_ratio=0.9, val_interval=5, weight_decay=0.0006)\n",
      "start to process task vehicleid_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.6987 ktau: 0.0141\n",
      "[Validate] Epoch 1: ktau: 0.2700\n",
      "[Train] Epoch 100: Loss: 0.3908 ktau: 0.6463\n",
      "[Validate] Epoch 100: ktau: 0.7400\n",
      "[Train] Epoch 200: Loss: 0.3364 ktau: 0.6989\n",
      "[Validate] Epoch 200: ktau: 0.7500\n",
      "[Train] Epoch 300: Loss: 0.3095 ktau: 0.7359\n",
      "[Validate] Epoch 300: ktau: 0.7374\n",
      "[Train] Epoch 400: Loss: 0.3089 ktau: 0.7300\n",
      "[Validate] Epoch 400: ktau: 0.7033\n",
      "Best train KTau: 0.793333@epoch 115 on task vehicleid_rank\n",
      "Namespace(batch_size=25, cos=True, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.0005, num_epochs=400, num_workers=4, save_name='train_final', seed=4, train_ratio=0.9, val_interval=1, weight_decay=0.0006)\n",
      "start to process task dukemtmc_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.6987 ktau: -0.0456\n",
      "[Validate] Epoch 1: ktau: -0.0300\n",
      "[Train] Epoch 100: Loss: 0.1954 ktau: 0.8537\n",
      "[Validate] Epoch 100: ktau: 0.9167\n",
      "[Train] Epoch 200: Loss: 0.1869 ktau: 0.8611\n",
      "[Validate] Epoch 200: ktau: 0.8867\n",
      "[Train] Epoch 300: Loss: 0.1944 ktau: 0.8500\n",
      "[Validate] Epoch 300: ktau: 0.9300\n",
      "[Train] Epoch 400: Loss: 0.1811 ktau: 0.8537\n",
      "[Validate] Epoch 400: ktau: 0.9300\n",
      "Best train KTau: 0.946667@epoch 366 on task dukemtmc_rank\n",
      "Namespace(batch_size=32, cos=False, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='train_final', seed=1, train_ratio=0.9, val_interval=1, weight_decay=0.0006)\n",
      "start to process task market1501_rank\n",
      "before dataloader init tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.5228,  0.3817, -1.0276],\n",
      "        [-0.5631, -0.8923, -0.0583]])\n",
      "[Train] Epoch 1: Loss: 0.7017 ktau: 0.0395\n",
      "[Validate] Epoch 1: ktau: 0.0806\n",
      "[Train] Epoch 100: Loss: 0.2546 ktau: 0.7990\n",
      "[Validate] Epoch 100: ktau: 0.8347\n",
      "[Train] Epoch 200: Loss: 0.2566 ktau: 0.7785\n",
      "[Validate] Epoch 200: ktau: 0.8133\n",
      "[Train] Epoch 300: Loss: 0.2386 ktau: 0.8090\n",
      "[Validate] Epoch 300: ktau: 0.7863\n",
      "[Train] Epoch 400: Loss: 0.2386 ktau: 0.8125\n",
      "[Validate] Epoch 400: ktau: 0.7863\n",
      "Best train KTau: 0.891129@epoch 62 on task market1501_rank\n",
      "Namespace(batch_size=32, cos=True, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='train_final', seed=1, train_ratio=0.8, val_interval=5, weight_decay=0.0006)\n",
      "start to process task msmt17_rank\n",
      "before dataloader init tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.5228,  0.3817, -1.0276],\n",
      "        [-0.5631, -0.8923, -0.0583]])\n",
      "[Train] Epoch 1: Loss: 0.6955 ktau: -0.0024\n",
      "[Validate] Epoch 1: ktau: 0.0739\n",
      "[Train] Epoch 100: Loss: 0.1969 ktau: 0.8572\n",
      "[Validate] Epoch 100: ktau: 0.9600\n",
      "[Train] Epoch 200: Loss: 0.1941 ktau: 0.8612\n",
      "[Validate] Epoch 200: ktau: 0.9570\n",
      "[Train] Epoch 300: Loss: 0.1725 ktau: 0.8726\n",
      "[Validate] Epoch 300: ktau: 0.9597\n",
      "[Train] Epoch 400: Loss: 0.1638 ktau: 0.8935\n",
      "[Validate] Epoch 400: ktau: 0.9677\n",
      "Best train KTau: 0.970430@epoch 95 on task msmt17_rank\n",
      "Namespace(batch_size=32, cos=True, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='train_final', seed=0, train_ratio=0.8, val_interval=5, weight_decay=0.0006)\n",
      "start to process task veri_rank\n",
      "before dataloader init tensor([[ 1.5410, -0.2934, -2.1788],\n",
      "        [ 0.5684, -1.0845, -1.3986]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[ 0.4033,  0.8380, -0.7193],\n",
      "        [-0.4033, -0.5966,  0.1820]])\n",
      "[Train] Epoch 1: Loss: 0.6979 ktau: -0.0242\n",
      "[Validate] Epoch 1: ktau: 0.3024\n",
      "[Train] Epoch 100: Loss: 0.2616 ktau: 0.8122\n",
      "[Validate] Epoch 100: ktau: 0.8804\n",
      "[Train] Epoch 200: Loss: 0.2317 ktau: 0.8044\n",
      "[Validate] Epoch 200: ktau: 0.8938\n",
      "[Train] Epoch 300: Loss: 0.2164 ktau: 0.8303\n",
      "[Validate] Epoch 300: ktau: 0.8804\n",
      "[Train] Epoch 400: Loss: 0.2593 ktau: 0.8034\n",
      "[Validate] Epoch 400: ktau: 0.8981\n",
      "Best train KTau: 0.927419@epoch 110 on task veri_rank\n",
      "Namespace(batch_size=32, cos=True, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='train_final', seed=3, train_ratio=0.8, val_interval=1, weight_decay=0.0006)\n",
      "start to process task veriwild_rank\n",
      "before dataloader init tensor([[ 0.8033,  0.1748,  0.0890],\n",
      "        [-0.6137,  0.0462, -1.3683]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[ 0.3375,  1.0111, -1.4352],\n",
      "        [ 0.9774,  0.5220,  1.2379]])\n",
      "[Train] Epoch 1: Loss: 0.6954 ktau: 0.0034\n",
      "[Validate] Epoch 1: ktau: -0.0954\n",
      "[Train] Epoch 100: Loss: 0.2667 ktau: 0.7903\n",
      "[Validate] Epoch 100: ktau: 0.9180\n",
      "[Train] Epoch 200: Loss: 0.2075 ktau: 0.8370\n",
      "[Validate] Epoch 200: ktau: 0.9244\n",
      "[Train] Epoch 300: Loss: 0.2375 ktau: 0.8041\n",
      "[Validate] Epoch 300: ktau: 0.9405\n",
      "[Train] Epoch 400: Loss: 0.2227 ktau: 0.8263\n",
      "[Validate] Epoch 400: ktau: 0.9331\n",
      "Best train KTau: 0.948218@epoch 195 on task veriwild_rank\n",
      "Namespace(batch_size=50, cos=True, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='train_final', seed=4, train_ratio=0.8, val_interval=1, weight_decay=0.0006)\n",
      "start to process task sop_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.6921 ktau: 0.0545\n",
      "[Validate] Epoch 1: ktau: 0.2408\n",
      "[Train] Epoch 100: Loss: 0.3229 ktau: 0.7447\n",
      "[Validate] Epoch 100: ktau: 0.8082\n",
      "[Train] Epoch 200: Loss: 0.2813 ktau: 0.7629\n",
      "[Validate] Epoch 200: ktau: 0.8156\n",
      "[Train] Epoch 300: Loss: 0.2553 ktau: 0.8010\n",
      "[Validate] Epoch 300: ktau: 0.8180\n",
      "[Train] Epoch 400: Loss: 0.2619 ktau: 0.7912\n",
      "[Validate] Epoch 400: ktau: 0.8075\n",
      "Best train KTau: 0.833469@epoch 194 on task sop_rank\n"
     ]
    }
   ],
   "source": [
    "# 1 running the bash scripts\n",
    "!bash train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 running and debugging in the notebook\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset import ArchPerfDataset\n",
    "from network import AutoEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed, ranking based loss, norm the final relative scores to rankings\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.enabled = True\n",
    "    cudnn.deterministic = True \n",
    "\n",
    "def pair_loss(outputs, labels): # output.shape = torch.Size([1023]) labels = torch.Size([1023])\n",
    "    \n",
    "    output = outputs.unsqueeze(1)\n",
    "    output1 = output.repeat(1,outputs.shape[0])\n",
    "    label = labels.unsqueeze(1)\n",
    "    label1 = label.repeat(1,labels.shape[0])\n",
    "    tmp = (output1-output1.t())*torch.sign(label1-label1.t())\n",
    "    tmp = torch.log(1+torch.exp(-tmp))\n",
    "    eye_tmp = tmp*torch.eye(len(tmp)).cuda()\n",
    "    new_tmp = tmp - eye_tmp\n",
    "    loss = torch.sum(new_tmp)/(outputs.shape[0]*(outputs.shape[0]-1))\n",
    "    return loss\n",
    "def norm_list(scores):\n",
    "    scores_ls_sort=scores.tolist()\n",
    "    scores_ls=scores.tolist()\n",
    "    scores_ls_sort.sort()\n",
    "    rank_number=[]\n",
    "    for item in scores_ls:\n",
    "        rank=scores_ls_sort.index(item)\n",
    "        rank_number.append(rank)\n",
    "    return rank_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch train,val and test\n",
    "def train_epoch(model, criterion, optimizer, train_loader, epoch, log_interval):\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_ktau = 0\n",
    "    for step, (archs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        archs, targets = archs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(archs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        # outputs.shape=torch.Size([16]) targets.shape = torch.Size([16])\n",
    "        # loss = criterion(outputs, targets)\n",
    "        loss = pair_loss(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ktau, p_value = stats.kendalltau(outputs.detach().cpu().numpy(), targets.detach().cpu().numpy())\n",
    "        running_ktau += ktau\n",
    "        running_loss += loss.item()*archs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss/(len(train_loader)*archs.size(0))\n",
    "    epoch_ktau = running_ktau/(step+1)\n",
    "\n",
    "    if (epoch+1)%log_interval ==0 or epoch ==0:\n",
    "        print('[Train] Epoch {}: Loss: {:.4f} ktau: {:.4f}'.format(epoch+1, epoch_loss, epoch_ktau))\n",
    "\n",
    "    return epoch_loss, epoch_ktau\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(model, val_loader, epoch, log_interval):\n",
    "\n",
    "    running_ktau = 0\n",
    "    for step, (archs, targets) in enumerate(val_loader):\n",
    "        \n",
    "        archs, targets = archs.cuda(), targets.cuda()\n",
    "\n",
    "        outputs = model(archs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        ktau, p_value = stats.kendalltau(outputs.detach().cpu().numpy(), targets.detach().cpu().numpy())\n",
    "        running_ktau += ktau\n",
    "\n",
    "    epoch_ktau = running_ktau/(step+1)\n",
    "    if (epoch+1)%log_interval ==0 or epoch ==0:\n",
    "        print('[Validate] Epoch {}: ktau: {:.4f}'.format(epoch+1, epoch_ktau))\n",
    "        \n",
    "    return epoch_ktau\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, test_loader):\n",
    "    \n",
    "    total_output = []\n",
    "    for step, archs in enumerate(test_loader):\n",
    "\n",
    "        archs = archs.cuda()\n",
    "\n",
    "        outputs = model(archs)\n",
    "        outputs = list(outputs.squeeze(1).detach().cpu().numpy())\n",
    "        total_output = total_output + outputs\n",
    "\n",
    "    return total_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(target_type, args):\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    g_cpu = torch.Generator()\n",
    "    g_cpu.manual_seed(args.seed)\n",
    "\n",
    "    print('before dataloader init',torch.randn(2,3))\n",
    "\n",
    "    train_data = ArchPerfDataset(root=args.data_path, target_type=target_type, train=True, encode_dimension=args.encode_dimension)\n",
    "    test_data = ArchPerfDataset(root=args.data_path, target_type=target_type, train=False, encode_dimension=args.encode_dimension)\n",
    "\n",
    "    indices = list(range(len(train_data)))\n",
    "    split = int(np.floor(args.train_ratio*len(train_data)))\n",
    "\n",
    "    train_loader = DataLoader(train_data, \n",
    "                            batch_size=args.batch_size,\n",
    "                            sampler=SubsetRandomSampler(indices[:split],generator=g_cpu),\n",
    "                            pin_memory=True,\n",
    "                            num_workers=args.num_workers,\n",
    "                            drop_last=True\n",
    "                            )\n",
    "    \n",
    "    val_loader = DataLoader(train_data, \n",
    "                            batch_size=args.batch_size,\n",
    "                            sampler=SubsetRandomSampler(indices[split:],generator=g_cpu),\n",
    "                            pin_memory=True,\n",
    "                            num_workers=args.num_workers,\n",
    "                            drop_last=True\n",
    "                            )\n",
    "\n",
    "    test_loader = DataLoader(test_data, \n",
    "                            batch_size=args.batch_size,\n",
    "                            pin_memory=True,\n",
    "                            shuffle=False,\n",
    "                            num_workers=args.num_workers,\n",
    "                            drop_last=False\n",
    "                            )\n",
    "    print('after dataloader init',torch.randn(2,3))\n",
    "\n",
    "    model = AutoEncoder(dropout=args.dropout_ratio).cuda()\n",
    "\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = None\n",
    "    if args.cos:\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, float(args.num_epochs))\n",
    "\n",
    "    best_ktau = 0\n",
    "    best_epoch = 0\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    all_model_weights = {}\n",
    "    \n",
    "    for eps in range(args.num_epochs):\n",
    "        flag = '{}_train'.format(target_type)\n",
    "        model.train()\n",
    "        train_epoch_loss, train_epoch_ktau = train_epoch(model, criterion, optimizer, train_loader, eps, args.log_interval)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        if (eps+1)%args.val_interval == 0 or eps==0:\n",
    "            flag = '{}_validate'.format(target_type)\n",
    "            model.eval()\n",
    "            epoch_ktau = val_epoch(model, val_loader, eps, args.log_interval)\n",
    "\n",
    "            if epoch_ktau > best_ktau:\n",
    "                best_ktau = epoch_ktau\n",
    "                best_epoch = eps+1\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # save model from every epoch\n",
    "            all_model_weights[epoch_ktau] = copy.deepcopy(model.state_dict())\n",
    "    torch.save(all_model_weights, './results/{}_epoch_inteval{}.pth'.format(target_type, args.val_interval))\n",
    "\n",
    "    print('Best train KTau: {:4f}@epoch {} on task {}'.format(best_ktau, best_epoch, target_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, cos=False, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=1, train_ratio=0.9, val_interval=1, weight_decay=0.0003)\n"
     ]
    }
   ],
   "source": [
    "# set default parameters\n",
    "parser = argparse.ArgumentParser(description='PyTorch Estimator Training')\n",
    "parser.add_argument('--data_path', type=str, default='./data/', help='dataset path')\n",
    "parser.add_argument('--train_ratio', type=float, default=0.9, help='ratio of train data')\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4)\n",
    "parser.add_argument('--num_epochs', type=int, default=400)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--log_interval', type=int, default=100)\n",
    "# parser.add_argument('--target_type', type=str, default='market1501_rank', help='8 target missions')\n",
    "parser.add_argument('--num_workers', type=int, default=4)\n",
    "parser.add_argument('--save_name', type=str, default='exp1')\n",
    "parser.add_argument('--encode_dimension', type=int, default=11)\n",
    "parser.add_argument('--dropout_ratio', type=float, default=0.5)\n",
    "parser.add_argument('--cos', action='store_true', default=False)\n",
    "parser.add_argument('--val_interval', type=int, default=1)\n",
    "\n",
    "args = parser.parse_args(args=['--data_path', './data/'])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions for hyper-parameters tunning:\n",
    "1. Low-ranking task like cplfw which has a ktau around the 0.3, indicates that the ViT model config has little to do with the final performance over such task. And this yields a hard optimization. In this situation, we select the small batch size (8 for cplfw) because we are afraid that a larger batch size would easily miss the sharp optimal point. Besides, the lr is fixed and not annealing to zero because we want the optimization process to have more chances to find the optimal points rather than falling into one sub-optimal point. This is certified during the hyper-parameters selection procedure.\n",
    "\n",
    "2. For high-ranking tasks, the optimization space is more smooth and easier to locate the optimal, so we adopt a larger batch size and enable the cos-annealing lr scheduling.\n",
    "\n",
    "3. Since the given training and validating samples are limited (500), the validating ktau is not trustful for selecting the final predictors to eval over the large search space (99500). So we save all the checkpoints on every n epoch (given in args.val_interval) and sort them according to the validation ktau. Then in the testing stage, we traverse the checkpoints with top ktau and obtain the final results on the test set. Finally, we upload and select the best one on board B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=8, cos=False, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=4, train_ratio=0.7, val_interval=1, weight_decay=0.0006)\n",
      "start to process task cplfw_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.7015 ktau: -0.0664\n",
      "[Validate] Epoch 1: ktau: 0.1667\n",
      "[Train] Epoch 100: Loss: 0.5932 ktau: 0.3854\n",
      "[Validate] Epoch 100: ktau: 0.2698\n",
      "[Train] Epoch 200: Loss: 0.5742 ktau: 0.4086\n",
      "[Validate] Epoch 200: ktau: 0.3254\n",
      "[Train] Epoch 300: Loss: 0.5554 ktau: 0.4286\n",
      "[Validate] Epoch 300: ktau: 0.3351\n",
      "[Train] Epoch 400: Loss: 0.5661 ktau: 0.4070\n",
      "[Validate] Epoch 400: ktau: 0.2778\n",
      "Best train KTau: 0.395096@epoch 348 on task cplfw_rank\n",
      "Namespace(batch_size=25, cos=True, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=4, train_ratio=0.9, val_interval=5, weight_decay=0.0006)\n",
      "start to process task vehicleid_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.6987 ktau: 0.0141\n",
      "[Validate] Epoch 1: ktau: 0.2700\n",
      "[Train] Epoch 100: Loss: 0.3908 ktau: 0.6463\n",
      "[Validate] Epoch 100: ktau: 0.7400\n",
      "[Train] Epoch 200: Loss: 0.3364 ktau: 0.6989\n",
      "[Validate] Epoch 200: ktau: 0.7500\n",
      "[Train] Epoch 300: Loss: 0.3095 ktau: 0.7359\n",
      "[Validate] Epoch 300: ktau: 0.7374\n",
      "[Train] Epoch 400: Loss: 0.3089 ktau: 0.7300\n",
      "[Validate] Epoch 400: ktau: 0.7033\n",
      "Best train KTau: 0.793333@epoch 115 on task vehicleid_rank\n",
      "Namespace(batch_size=25, cos=True, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.0005, num_epochs=400, num_workers=4, save_name='exp1', seed=4, train_ratio=0.9, val_interval=1, weight_decay=0.0006)\n",
      "start to process task dukemtmc_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.6987 ktau: -0.0456\n",
      "[Validate] Epoch 1: ktau: -0.0300\n",
      "[Train] Epoch 100: Loss: 0.1954 ktau: 0.8537\n",
      "[Validate] Epoch 100: ktau: 0.9167\n",
      "[Train] Epoch 200: Loss: 0.1869 ktau: 0.8611\n",
      "[Validate] Epoch 200: ktau: 0.8867\n",
      "[Train] Epoch 300: Loss: 0.1944 ktau: 0.8500\n",
      "[Validate] Epoch 300: ktau: 0.9300\n",
      "[Train] Epoch 400: Loss: 0.1811 ktau: 0.8537\n",
      "[Validate] Epoch 400: ktau: 0.9300\n",
      "Best train KTau: 0.946667@epoch 366 on task dukemtmc_rank\n",
      "Namespace(batch_size=32, cos=False, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=1, train_ratio=0.9, val_interval=1, weight_decay=0.0006)\n",
      "start to process task market1501_rank\n",
      "before dataloader init tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.5228,  0.3817, -1.0276],\n",
      "        [-0.5631, -0.8923, -0.0583]])\n",
      "[Train] Epoch 1: Loss: 0.7017 ktau: 0.0395\n",
      "[Validate] Epoch 1: ktau: 0.0806\n",
      "[Train] Epoch 100: Loss: 0.2546 ktau: 0.7990\n",
      "[Validate] Epoch 100: ktau: 0.8347\n",
      "[Train] Epoch 200: Loss: 0.2566 ktau: 0.7785\n",
      "[Validate] Epoch 200: ktau: 0.8133\n",
      "[Train] Epoch 300: Loss: 0.2386 ktau: 0.8090\n",
      "[Validate] Epoch 300: ktau: 0.7863\n",
      "[Train] Epoch 400: Loss: 0.2386 ktau: 0.8125\n",
      "[Validate] Epoch 400: ktau: 0.7863\n",
      "Best train KTau: 0.891129@epoch 62 on task market1501_rank\n",
      "Namespace(batch_size=32, cos=True, data_path='./data/', dropout_ratio=0.4, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=1, train_ratio=0.8, val_interval=5, weight_decay=0.0006)\n",
      "start to process task msmt17_rank\n",
      "before dataloader init tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.5228,  0.3817, -1.0276],\n",
      "        [-0.5631, -0.8923, -0.0583]])\n",
      "[Train] Epoch 1: Loss: 0.6955 ktau: -0.0024\n",
      "[Validate] Epoch 1: ktau: 0.0739\n",
      "[Train] Epoch 100: Loss: 0.1969 ktau: 0.8572\n",
      "[Validate] Epoch 100: ktau: 0.9600\n",
      "[Train] Epoch 200: Loss: 0.1941 ktau: 0.8612\n",
      "[Validate] Epoch 200: ktau: 0.9570\n",
      "[Train] Epoch 300: Loss: 0.1725 ktau: 0.8726\n",
      "[Validate] Epoch 300: ktau: 0.9597\n",
      "[Train] Epoch 400: Loss: 0.1638 ktau: 0.8935\n",
      "[Validate] Epoch 400: ktau: 0.9677\n",
      "Best train KTau: 0.970430@epoch 95 on task msmt17_rank\n",
      "Namespace(batch_size=32, cos=True, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=0, train_ratio=0.8, val_interval=5, weight_decay=0.0006)\n",
      "start to process task veri_rank\n",
      "before dataloader init tensor([[ 1.5410, -0.2934, -2.1788],\n",
      "        [ 0.5684, -1.0845, -1.3986]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[ 0.4033,  0.8380, -0.7193],\n",
      "        [-0.4033, -0.5966,  0.1820]])\n",
      "[Train] Epoch 1: Loss: 0.6979 ktau: -0.0242\n",
      "[Validate] Epoch 1: ktau: 0.3024\n",
      "[Train] Epoch 100: Loss: 0.2616 ktau: 0.8122\n",
      "[Validate] Epoch 100: ktau: 0.8804\n",
      "[Train] Epoch 200: Loss: 0.2317 ktau: 0.8044\n",
      "[Validate] Epoch 200: ktau: 0.8938\n",
      "[Train] Epoch 300: Loss: 0.2164 ktau: 0.8303\n",
      "[Validate] Epoch 300: ktau: 0.8804\n",
      "[Train] Epoch 400: Loss: 0.2593 ktau: 0.8034\n",
      "[Validate] Epoch 400: ktau: 0.8981\n",
      "Best train KTau: 0.927419@epoch 110 on task veri_rank\n",
      "Namespace(batch_size=32, cos=True, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=3, train_ratio=0.8, val_interval=1, weight_decay=0.0006)\n",
      "start to process task veriwild_rank\n",
      "before dataloader init tensor([[ 0.8033,  0.1748,  0.0890],\n",
      "        [-0.6137,  0.0462, -1.3683]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[ 0.3375,  1.0111, -1.4352],\n",
      "        [ 0.9774,  0.5220,  1.2379]])\n",
      "[Train] Epoch 1: Loss: 0.6954 ktau: 0.0034\n",
      "[Validate] Epoch 1: ktau: -0.0954\n",
      "[Train] Epoch 100: Loss: 0.2667 ktau: 0.7903\n",
      "[Validate] Epoch 100: ktau: 0.9180\n",
      "[Train] Epoch 200: Loss: 0.2075 ktau: 0.8370\n",
      "[Validate] Epoch 200: ktau: 0.9244\n",
      "[Train] Epoch 300: Loss: 0.2375 ktau: 0.8041\n",
      "[Validate] Epoch 300: ktau: 0.9405\n",
      "[Train] Epoch 400: Loss: 0.2227 ktau: 0.8263\n",
      "[Validate] Epoch 400: ktau: 0.9331\n",
      "Best train KTau: 0.948218@epoch 195 on task veriwild_rank\n",
      "Namespace(batch_size=50, cos=True, data_path='./data/', dropout_ratio=0.5, encode_dimension=11, gpu=0, log_interval=100, lr=0.001, num_epochs=400, num_workers=4, save_name='exp1', seed=4, train_ratio=0.8, val_interval=1, weight_decay=0.0006)\n",
      "start to process task sop_rank\n",
      "before dataloader init tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_train.json\n",
      "load data from: ./data/CVPR_2022_NAS_Track2_test.json\n",
      "after dataloader init tensor([[-1.4260,  0.9039,  0.8557],\n",
      "        [ 0.6889,  0.8850,  1.7706]])\n",
      "[Train] Epoch 1: Loss: 0.6921 ktau: 0.0545\n",
      "[Validate] Epoch 1: ktau: 0.2408\n",
      "[Train] Epoch 100: Loss: 0.3229 ktau: 0.7447\n",
      "[Validate] Epoch 100: ktau: 0.8082\n",
      "[Train] Epoch 200: Loss: 0.2813 ktau: 0.7629\n",
      "[Validate] Epoch 200: ktau: 0.8156\n",
      "[Train] Epoch 300: Loss: 0.2553 ktau: 0.8010\n",
      "[Validate] Epoch 300: ktau: 0.8180\n",
      "[Train] Epoch 400: Loss: 0.2619 ktau: 0.7912\n",
      "[Validate] Epoch 400: ktau: 0.8075\n",
      "Best train KTau: 0.833469@epoch 194 on task sop_rank\n"
     ]
    }
   ],
   "source": [
    "# running 8 predictor training on 8 tasks\n",
    "\n",
    "task_list = [\"cplfw_rank\",\"vehicleid_rank\",\"dukemtmc_rank\",\"market1501_rank\",\"msmt17_rank\",\"veri_rank\",\"veriwild_rank\",\"sop_rank\"]\n",
    "\n",
    "with open('./data/CVPR_2022_NAS_Track2_test.json', 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "for data_type in task_list:\n",
    "        if data_type == 'cplfw_rank':\n",
    "            # best: lr=1e-3, wd=6e-4 bsz=8, ratio=0.7, seed=4, dp=0.4, cos=False, val_interva=1\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 8\n",
    "            args.train_ratio = 0.7\n",
    "            args.seed = 4\n",
    "            args.dropout_ratio = 0.4\n",
    "            args.cos=False\n",
    "            args.val_interval = 1\n",
    "        elif data_type == 'vehicleid_rank':\n",
    "            # current best: lr=1e-3, wd=6e-4 bsz=25, ratio=0.9, seed=4, dp=0.4, cos=True, val_inter=5\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 25\n",
    "            args.train_ratio = 0.9\n",
    "            args.seed = 4\n",
    "            args.dropout_ratio = 0.4\n",
    "            args.cos=True\n",
    "            args.val_interval = 5\n",
    "        elif data_type == 'dukemtmc_rank':\n",
    "            # current best: lr=5e-4, wd=6e-4 bsz=25, ratio=0.9, seed=4, dp=0.4, cos=True, val_interval=1\n",
    "            args.lr = 5e-4\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 25\n",
    "            args.train_ratio = 0.9\n",
    "            args.seed=4\n",
    "            args.dropout_ratio=0.4\n",
    "            args.cos=True\n",
    "            args.val_interval = 1\n",
    "        elif data_type == 'market1501_rank':\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 32\n",
    "            args.train_ratio = 0.9\n",
    "            args.seed=1\n",
    "            args.dropout_ratio=0.5\n",
    "            args.cos=False\n",
    "            args.val_interval = 1\n",
    "        elif data_type == 'msmt17_rank':\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 32\n",
    "            args.train_ratio = 0.8\n",
    "            args.seed=1\n",
    "            args.dropout_ratio=0.4\n",
    "            args.cos=True\n",
    "            args.val_interval = 5\n",
    "        elif data_type == 'veri_rank':\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 32\n",
    "            args.train_ratio = 0.8\n",
    "            args.seed=0\n",
    "            args.dropout_ratio=0.5\n",
    "            args.cos=True\n",
    "            args.val_interval = 5\n",
    "        elif data_type == 'veriwild_rank':\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 32\n",
    "            args.train_ratio = 0.8\n",
    "            args.seed=3\n",
    "            args.dropout_ratio=0.5\n",
    "            args.cos=True\n",
    "            args.val_interval = 1\n",
    "        elif data_type == 'sop_rank':\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = 6e-4\n",
    "            args.batch_size = 50\n",
    "            args.train_ratio = 0.8\n",
    "            args.seed=4\n",
    "            args.dropout_ratio=0.5\n",
    "            args.cos=True\n",
    "            args.val_interval = 1\n",
    "        print(args)\n",
    "\n",
    "        print('start to process task {}'.format(data_type))\n",
    "        \n",
    "        main(data_type, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model checkpoints are stored in the dir, please run the test file to load the checkpoints and eval."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "303f209d9515745736caecbf6e28d85f87a490a49e3651540a01ca7db04ef59c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rookie': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
